# 机器学习之决策树算法

## 决策树原理介绍
决策树是用样本的属性作为结点，用属性的取值作为分支的树结构，是通过一系列规则对数据进行分类的过程，它提供一种在什么条件下会得到什么值的类似规则的方法。决策树分为分类树和回归树两种，分类树对离散变量做决策树，回归树对连续变量做决策树。下图是一个相亲时候的决策图，女方根据男方的条件来决定是否见面（仅为解释）。
![](相亲图.png)


## 决策树的构造
一棵决策树的生成过程主要分为以下3个部分:
1. **特征选择**
特征选择是指从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，比如相亲时要根据男方的一系列特征[年龄，长相，收入，是否公务员]来决定见不见面，特征选择就是每次选择哪个特征才能得到一个最好的决策结果。如何选择特征有着很多不同量化评估标准标准，从而衍生出不同的决策树算法。
2. **决策树生成**
根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则停止决策树停止生长。
3. **剪枝**
决策树容易过拟合，一般来需要剪枝，缩小树结构规模、缓解过拟合。剪枝技术有预剪枝和后剪枝两种。

在构造决策树时，需要解决的第一个问题是，当前数据集上那个特征在划分数据分类时起决定作用。为找到决定性的特征，划分出最好的结果，需要评估每个特征。当根据第一个特征进行数据划分后，原始数据集会分布在第一个决策点的所有分支上。如果某个分支下的数据属于同一类型，则该分支无需继续划分。如果数据子集内的数据不属于同一类型则需要根据新的特征继续划分。
创建分支的伪代码如下：
```
def create_branch():
    根据数据集中的每个子项是否属于同一分类:
        if so:
            return 类标签
        else:
            寻找划分数据集的最好特征
            划分数据集
            创建分支节点
            for 每个划分的子集:
                create_branch()
            return 分支节点
            
```

### 信息熵
每次划分数据集时如何选择合适的特征才能产生最优的划分结果，可以用信息熵来评估。在概率论中，信息熵给了我们一种度量不确定性的方式，是用来衡量随机变量不确定性的，**熵就是信息的期望值**。若待分类的事物可能划分在N类中，分别是x1，x2，……，xn，每一种取到的概率分别是P1，P2，……，Pn，那么X的熵就定义为：
$$ H(X) = -\sum_{i=1}^{n} p_i log_2 p_i $$
熵越高就表明数据越混乱，不确定性越高。

举个例子，掷一枚筛子的结果的信息熵$ H = -\sum_{i=1}^{6} \frac{1}{6} log_2 \frac{1}{6} =  2.58 $。

划分数据集的最大原则是：将无序的数据变得更加有序。划分数据集前后信息熵发生的变化叫做信息增益，信息增益是熵的减少或数据无序度的减少，获得信息增益最高的特征就是最好的选择。
`Gain(A) = Info(D) - Info(D-a)`

使用特征a进行划分后的信息增益 =（原始数据的信息熵）-（使用特征a进行划分后得到的新数据集的信息熵）。

### 划分数据集
有了信息熵的概念，我们就可以依此来选择特征划分数据集了。具体过程是遍历数据集的每个特征，用每个特征的不同值去划分数据集，计算信息增益。最后返回信息增益最大的那个特征。

### 递归构建决策树
得到原始数据集，基于最好特征划分一次数据集。第一次划分后，数据将被向下传递到树分支的下一节点，在这个节点熵再次划分数据。递归结束的条件是程序遍历完所有属性或者每个分之下的所有实力都属于同一分类。
